from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from tgb_seq.LinkPred.dataloader import TGBSeqLoader
from tgb.linkproppred.dataset import LinkPropPredDataset
import os

TGB_Seq_DATA_NAME=["ML-20M", "Taobao", "Yelp","GoogleLocal", "Flickr", "YouTube", "Patent", "WikiLink"]
TGB_DATA_NAME = ["tgbl-wiki","tgbl-review", "tgbl-coin", "tgbl-flight", "tgbl-comment"]

class CustomizedDataset(Dataset):
    def __init__(self, indices_list: list):
        """
        Customized dataset.
        :param indices_list: list, list of indices
        """
        super(CustomizedDataset, self).__init__()

        self.indices_list = indices_list

    def __getitem__(self, idx: int):
        """
        get item at the index in self.indices_list
        :param idx: int, the index
        :return:
        """
        return self.indices_list[idx]

    def __len__(self):
        return len(self.indices_list)


def get_idx_data_loader(indices_list: list, batch_size: int, shuffle: bool):
    """
    get data loader that iterates over indices
    :param indices_list: list, list of indices
    :param batch_size: int, batch size
    :param shuffle: boolean, whether to shuffle the data
    :return: data_loader, DataLoader
    """
    dataset = CustomizedDataset(indices_list=indices_list)

    data_loader = DataLoader(dataset=dataset,
                             batch_size=batch_size,
                             shuffle=shuffle,
                             drop_last=False)
    return data_loader


class Data:

    def __init__(self, src_node_ids: np.ndarray, dst_node_ids: np.ndarray, node_interact_times: np.ndarray, edge_ids: np.ndarray, labels: np.ndarray, neg_samples: np.ndarray = None, split=None ):
        """
        Data object to store the nodes interaction information.
        :param src_node_ids: ndarray
        :param dst_node_ids: ndarray
        :param node_interact_times: ndarray
        :param edge_ids: ndarray
        :param labels: ndarray
        # ! node_id must start from 1. edge_id must start from 1
        """
        self.src_node_ids = src_node_ids
        self.dst_node_ids = dst_node_ids
        self.node_interact_times = node_interact_times
        self.edge_ids = edge_ids
        self.labels = labels
        self.num_interactions = len(src_node_ids)
        self.unique_node_ids = set(src_node_ids) | set(dst_node_ids)
        self.num_unique_nodes = len(self.unique_node_ids)
        self.split=split
        self.max_node_id = max(src_node_ids.max(), dst_node_ids.max())
        if neg_samples is not None:
            self.neg_samples = neg_samples
            self.num_neg_samples = neg_samples.shape[1]
            # check if the number of negative samples matches the number of test samples, where the `split` is 2
            if split is not None:
                assert np.where(split==2)[0].shape[0] == neg_samples.shape[0], 'Number of negative samples does not match the number of test samples!'
            else:
                assert len(src_node_ids) == neg_samples.shape[0], 'Number of negative samples does not match the number of test samples!'
        else:
            self.neg_samples = None
            self.num_neg_samples = 0



def get_link_prediction_data(dataset_name: str, val_ratio: float = 0.15, test_ratio: float = 0.15, dataset_path='./processed_data/', use_edge_feat=False,use_node_feat=False, logger=None):
    """
    generate data for link prediction task (inductive & transductive settings)
    :param dataset_name: str, dataset name
    :param val_ratio: float, validation data ratio
    :param test_ratio: float, test data ratio
    :return: node_raw_features, edge_raw_features, (np.ndarray),
            full_data, train_data, val_data, test_data, new_node_val_data, new_node_test_data, (Data object)
    """
    if dataset_name in TGB_Seq_DATA_NAME:
        data=TGBSeqLoader(dataset_name, dataset_path)
        src_node_ids,dst_node_ids,node_interact_times,test_ns=data.src_node_ids,data.dst_node_ids,data.node_interact_times,data.negative_samples
        if test_ns is not None:
            logger.info("Using negative samples given by TGB-Seq")
        else:
            logger.info("Using negative samples generated by random sampling")
        if data.edge_features is not None and use_edge_feat:
            edge_raw_features =data.edge_features
            edge_raw_features=np.concatenate([np.zeros((1,edge_raw_features.shape[1])),edge_raw_features],axis=0) # padding 0 for the first edge
            print("Using edge features from the dataset")
        else:
            edge_raw_features = np.zeros((len(src_node_ids)+1, 1))
            print("Using zero edge features")
        if data.node_features is not None and use_node_feat:
            node_raw_features = data.node_features
            node_raw_features=np.concatenate([np.zeros((1,node_raw_features.shape[1])),node_raw_features],axis=0)
            print("Using node features from the dataset")
        else:
            node_raw_features = np.zeros((len(set(src_node_ids) | set(dst_node_ids))+1, 1))
            print("Using zero node features")
        NODE_FEAT_DIM = EDGE_FEAT_DIM = 172
        assert NODE_FEAT_DIM >= node_raw_features.shape[1], f'Node feature dimension in dataset {dataset_name} is bigger than {NODE_FEAT_DIM}!'
        assert EDGE_FEAT_DIM >= edge_raw_features.shape[1], f'Edge feature dimension in dataset {dataset_name} is bigger than {EDGE_FEAT_DIM}!'
        # padding the features of edges and nodes to the same dimension (172 for all the datasets)
        if node_raw_features.shape[1] < NODE_FEAT_DIM:
            node_zero_padding = np.zeros((node_raw_features.shape[0], NODE_FEAT_DIM - node_raw_features.shape[1]))
            node_raw_features = np.concatenate([node_raw_features, node_zero_padding], axis=1)
        if edge_raw_features.shape[1] < EDGE_FEAT_DIM:
            edge_zero_padding = np.zeros((edge_raw_features.shape[0], EDGE_FEAT_DIM - edge_raw_features.shape[1]))
            edge_raw_features = np.concatenate([edge_raw_features, edge_zero_padding], axis=1)

        assert NODE_FEAT_DIM == node_raw_features.shape[1] and EDGE_FEAT_DIM == edge_raw_features.shape[1], 'Unaligned feature dimensions after feature padding!'

        print('dataset_name',dataset_name)
        edge_ids = np.arange(1,len(src_node_ids)+1).astype(np.longlong)
        labels=np.zeros(len(src_node_ids))
        train_mask, val_mask, test_mask = data.train_mask,data.val_mask,data.test_mask
        full_data = Data(src_node_ids=src_node_ids, dst_node_ids=dst_node_ids, node_interact_times=node_interact_times, edge_ids=edge_ids, labels=labels)

        train_data = Data(src_node_ids=src_node_ids[train_mask], dst_node_ids=dst_node_ids[train_mask],node_interact_times=node_interact_times[train_mask],edge_ids=edge_ids[train_mask], labels=labels[train_mask])

        val_split = np.full(val_mask.sum(),1)
        val_data = Data(src_node_ids=src_node_ids[val_mask], dst_node_ids=dst_node_ids[val_mask],node_interact_times=node_interact_times[val_mask], edge_ids=edge_ids[val_mask], labels=labels[val_mask],split=val_split)

        test_split = np.full(test_mask.sum(),2)
        test_data = Data(src_node_ids=src_node_ids[test_mask], dst_node_ids=dst_node_ids[test_mask],node_interact_times=node_interact_times[test_mask], edge_ids=edge_ids[test_mask], labels=labels[test_mask],neg_samples=test_ns, split=test_split)

        val_test_mask = data.edgelist['split']!=0
        val_test_data= Data(src_node_ids=src_node_ids[val_test_mask], dst_node_ids=dst_node_ids[val_test_mask],
                        node_interact_times=node_interact_times[val_test_mask], edge_ids=edge_ids[val_test_mask], labels=labels[val_test_mask],neg_samples=test_ns, split=data.edgelist['split'][val_test_mask].values)
    elif dataset_name in TGB_DATA_NAME:
        # this part is mostly adopted from DyGLib_TGB
        dataset = LinkPropPredDataset(name=dataset_name, root=dataset_path, preprocess=True)
        data = dataset.full_data

        src_node_ids = data['sources'].astype(np.longlong)
        dst_node_ids = data['destinations'].astype(np.longlong)
        node_interact_times = data['timestamps'].astype(np.float64)
        edge_ids = data['edge_idxs'].astype(np.longlong)
        labels = data['edge_label']

        # union to get node set
        num_nodes = len(set(src_node_ids) | set(dst_node_ids))

        assert src_node_ids.min() == 0 or dst_node_ids.min() == 0, "Node index should start from 0!"
        assert edge_ids.min() == 0 or edge_ids.min() == 1, "Edge index should start from 0 or 1!"
        # we notice that the edge id on the datasets (except for tgbl-wiki) starts from 1, so we manually minus the edge ids by 1
        if edge_ids.min() == 1:
            print(f"Manually minus the edge indices by 1 on {dataset_name}")
            edge_ids = edge_ids - 1
        assert edge_ids.min() == 0, "After correction, edge index should start from 0!"

        train_mask = dataset.train_mask
        val_mask = dataset.val_mask
        test_mask = dataset.test_mask
        
        # Replace hyphen with underscore in dataset_name for file path construction
        processed_dataset_name = dataset_name.replace('-', '_')
        test_ns_path = os.path.join(dataset_path, processed_dataset_name, f"{processed_dataset_name}_test_ns.npy")
        # ! modifications: we store the negative samples for convenience of evaluation //////////////////////////////////////////////////////////////
        if os.path.exists(test_ns_path):
            test_ns = np.load(test_ns_path)
        else:
            num_negs = 0
            dataset.load_test_ns()
            test_ns_dict = dataset.ns_sampler.eval_set["test"]
            test_ns = []
            for src, dst, time in zip(src_node_ids[test_mask], dst_node_ids[test_mask], node_interact_times[test_mask]):
                if (src, dst, time) in test_ns_dict:
                    tmp_negs = test_ns_dict[(src, dst, time)]
                    if num_negs == 0:
                        num_negs = len(tmp_negs)
                    else:
                        assert num_negs == len(tmp_negs), f"Number of negative samples is not consistent! {num_negs} != {len(tmp_negs)}"
                    test_ns.append(tmp_negs)
                else:
                    raise KeyError("No such test sample")
            test_ns = np.array(test_ns)
            np.save(test_ns_path, test_ns)
        # ! end of modifications //////////////////////////////////////////////////////////////
        # note that in our data preprocess pipeline, we add an extra node and edge with index 0 as the padded node/edge for convenience of model computation,
        # therefore, for TGB, we also manually add the extra node and edge with index 0
        src_node_ids = src_node_ids + 1
        dst_node_ids = dst_node_ids + 1
        edge_ids = edge_ids + 1

        MAX_FEAT_DIM = 172
        if not use_node_feat:
            node_raw_features = np.zeros((num_nodes + 1, 1))
        else:
            if 'node_feat' not in data.keys():
                node_raw_features = np.zeros((num_nodes + 1, 1))
            else:
                node_raw_features = data['node_feat'].astype(np.float64)
                # deal with node features whose shape has only one dimension
                if len(node_raw_features.shape) == 1:
                    node_raw_features = node_raw_features[:, np.newaxis]
            if node_raw_features.shape[1] < MAX_FEAT_DIM:
                node_zero_padding = np.zeros((node_raw_features.shape[0], MAX_FEAT_DIM - node_raw_features.shape[1]))
                node_raw_features = np.concatenate([node_raw_features, node_zero_padding], axis=1)
            node_raw_features = np.vstack([np.zeros(node_raw_features.shape[1])[np.newaxis, :], node_raw_features])
            assert MAX_FEAT_DIM >= node_raw_features.shape[1], f'Node feature dimension in dataset {dataset_name} is bigger than {MAX_FEAT_DIM}!'
        if not use_edge_feat:
            edge_raw_features = np.zeros((len(src_node_ids) + 1, 1))
        else:
            edge_raw_features = data['edge_feat'].astype(np.float64)
            # deal with edge features whose shape has only one dimension
            if len(edge_raw_features.shape) == 1:
                edge_raw_features = edge_raw_features[:, np.newaxis]
            # currently, we do not consider edge weights
            # edge_weights = data['w'].astype(np.float64)
            if edge_raw_features.shape[1] < MAX_FEAT_DIM:
                edge_zero_padding = np.zeros((edge_raw_features.shape[0], MAX_FEAT_DIM - edge_raw_features.shape[1]))
                edge_raw_features = np.concatenate([edge_raw_features, edge_zero_padding], axis=1)
            edge_raw_features = np.vstack([np.zeros(edge_raw_features.shape[1])[np.newaxis, :], edge_raw_features])
            assert MAX_FEAT_DIM >= edge_raw_features.shape[1], f'Edge feature dimension in dataset {dataset_name} is bigger than {MAX_FEAT_DIM}!'

        full_data = Data(src_node_ids=src_node_ids, dst_node_ids=dst_node_ids, node_interact_times=node_interact_times, edge_ids=edge_ids, labels=labels)
        train_data = Data(src_node_ids=src_node_ids[train_mask], dst_node_ids=dst_node_ids[train_mask],
                        node_interact_times=node_interact_times[train_mask], edge_ids=edge_ids[train_mask], labels=labels[train_mask])
        val_split = np.full(val_mask.sum(),1)
        val_data = Data(src_node_ids=src_node_ids[val_mask], dst_node_ids=dst_node_ids[val_mask],
                        node_interact_times=node_interact_times[val_mask], edge_ids=edge_ids[val_mask], labels=labels[val_mask], split=val_split)
        test_split = np.full(test_mask.sum(),2)
        test_data = Data(src_node_ids=src_node_ids[test_mask], dst_node_ids=dst_node_ids[test_mask],
                        node_interact_times=node_interact_times[test_mask], edge_ids=edge_ids[test_mask], labels=labels[test_mask], neg_samples=test_ns, split=test_split)
        # the val_test_data will not be used
        val_test_mask = np.logical_or(val_mask, test_mask)
        val_test_split = np.array([1] * val_data.num_interactions + [2] * test_data.num_interactions)
        val_test_data = Data(src_node_ids=src_node_ids[val_test_mask], dst_node_ids=dst_node_ids[val_test_mask],
                        node_interact_times=node_interact_times[val_test_mask], edge_ids=edge_ids[val_test_mask], labels=labels[val_test_mask], split=val_test_split)
    else:
        # this part is adopted from DyGLib
        graph_df = pd.read_csv('{}/{}/ml_{}.csv'.format(dataset_path,dataset_name, dataset_name))
        edge_raw_features = np.load('{}/{}/ml_{}.npy'.format(dataset_path, dataset_name, dataset_name)) # In this file, the first row is the padding row of edge features
        node_raw_features = np.load('{}/{}/ml_{}_node.npy'.format(dataset_path, dataset_name, dataset_name)) # In this file, the first row is the padding row of node features

        NODE_FEAT_DIM = EDGE_FEAT_DIM = 172
        assert NODE_FEAT_DIM >= node_raw_features.shape[1], f'Node feature dimension in dataset {dataset_name} is bigger than {NODE_FEAT_DIM}!'
        assert EDGE_FEAT_DIM >= edge_raw_features.shape[1], f'Edge feature dimension in dataset {dataset_name} is bigger than {EDGE_FEAT_DIM}!'
        # padding the features of edges and nodes to the same dimension (172 for all the datasets)
        if node_raw_features.shape[1] < NODE_FEAT_DIM:
            node_zero_padding = np.zeros((node_raw_features.shape[0], NODE_FEAT_DIM - node_raw_features.shape[1]))
            node_raw_features = np.concatenate([node_raw_features, node_zero_padding], axis=1)
        if edge_raw_features.shape[1] < EDGE_FEAT_DIM:
            edge_zero_padding = np.zeros((edge_raw_features.shape[0], EDGE_FEAT_DIM - edge_raw_features.shape[1]))
            edge_raw_features = np.concatenate([edge_raw_features, edge_zero_padding], axis=1)

        assert NODE_FEAT_DIM == node_raw_features.shape[1] and EDGE_FEAT_DIM == edge_raw_features.shape[1], 'Unaligned feature dimensions after feature padding!'
        # get the timestamp of validate and test set
        val_time, test_time = list(np.quantile(graph_df.ts, [(1 - val_ratio - test_ratio), (1 - test_ratio)]))

        src_node_ids = graph_df.u.values.astype(np.longlong)
        dst_node_ids = graph_df.i.values.astype(np.longlong)
        node_interact_times = graph_df.ts.values.astype(np.float64)
        edge_ids = graph_df.idx.values.astype(np.longlong)
        labels = graph_df.label.values

        full_data = Data(src_node_ids=src_node_ids, dst_node_ids=dst_node_ids, node_interact_times=node_interact_times, edge_ids=edge_ids, labels=labels)

        # for train data, we keep edges happening before the validation time which do not involve any new node, used for inductiveness
        train_mask = node_interact_times <= val_time
        train_split = np.array([0] * sum(train_mask))
        train_data = Data(src_node_ids=src_node_ids[train_mask], dst_node_ids=dst_node_ids[train_mask],
                        node_interact_times=node_interact_times[train_mask],
                        edge_ids=edge_ids[train_mask], labels=labels[train_mask], split=train_split)

        val_mask = np.logical_and(node_interact_times <= test_time, node_interact_times > val_time)
        test_mask = node_interact_times > test_time

        # validation and test data
        val_split = np.array([1] * sum(val_mask))
        val_data = Data(src_node_ids=src_node_ids[val_mask], dst_node_ids=dst_node_ids[val_mask],
                        node_interact_times=node_interact_times[val_mask], edge_ids=edge_ids[val_mask], labels=labels[val_mask], split=val_split)
        
        test_split = np.array([2] * sum(test_mask))
        test_data = Data(src_node_ids=src_node_ids[test_mask], dst_node_ids=dst_node_ids[test_mask],
                        node_interact_times=node_interact_times[test_mask], edge_ids=edge_ids[test_mask], labels=labels[test_mask], split=test_split)
        
        # the val_test_data will not be used
        val_test_mask = node_interact_times > val_time
        val_test_split = np.array([1] * val_data.num_interactions + [2] * test_data.num_interactions)
        val_test_data = Data(src_node_ids=src_node_ids[val_test_mask], dst_node_ids=dst_node_ids[val_test_mask],
                        node_interact_times=node_interact_times[val_test_mask], edge_ids=edge_ids[val_test_mask], labels=labels[val_test_mask], split=val_test_split)
    logger.info("The dataset has {} interactions, involving {} different nodes".format(full_data.num_interactions, full_data.num_unique_nodes))
    logger.info("The training dataset has {} interactions, involving {} different nodes".format(
        train_data.num_interactions, train_data.num_unique_nodes))
    logger.info("The validation dataset has {} interactions, involving {} different nodes".format(
        val_data.num_interactions, val_data.num_unique_nodes))
    logger.info("The test dataset has {} interactions, involving {} different nodes".format(
        test_data.num_interactions, test_data.num_unique_nodes))
    
    return node_raw_features, edge_raw_features, full_data, train_data, val_data, test_data, None, None, val_test_data
